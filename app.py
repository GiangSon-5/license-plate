import gradio as gr
import cv2
import numpy as np
import easyocr
import math
import re
import string
from ultralytics import YOLO
import gc 

# 1. T·∫£i model EasyOCR
try:
    reader = easyocr.Reader(['en'])
    print("EasyOCR reader loaded successfully.")
except Exception as e:
    print(f"Error loading EasyOCR reader: {e}")
    reader = None

# 2. T·∫£i model YOLOv t·ª´ file .onnx
try:
    yolo_model = YOLO("best.onnx")
    print("YOLO ONNX model loaded successfully.")
except Exception as e:
    print(f"Error loading YOLO ONNX model 'best.onnx': {e}")
    print("Please make sure 'best.onnx' is in the same directory.")
    yolo_model = None

# --- C√°c h√†m x·ª≠ l√Ω ·∫£nh  ---

def rotate_image(image, angle):
    """Xoay ·∫£nh m·ªôt g√≥c (angle)"""
    image_center = tuple(np.array(image.shape[1::-1]) / 2)
    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)
    result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)
    return result

def compute_skew(src_img):
    """T√≠nh to√°n g√≥c nghi√™ng c·ªßa ·∫£nh"""
    if len(src_img.shape) == 3:
        h, w, _ = src_img.shape
    elif len(src_img.shape) == 2:
        h, w = src_img.shape
    else:
        print('Unsupported image type')
        return 0.0

    img = cv2.medianBlur(src_img, 3)
    edges = cv2.Canny(img, threshold1=50, threshold2=170, apertureSize=3, L2gradient=True)
    
    if h > 55:
        edges = edges[min(55, h-1):,:]

    lines = cv2.HoughLinesP(edges, 1, math.pi / 180, 30, minLineLength=w / 4.0, maxLineGap=h / 4.0)
    
    if lines is None:
        return 0.0

    angle = 0.0
    nlines = lines.shape[0]
    
    cnt = 0
    for x1, y1, x2, y2 in lines[:, 0]:
        ang = np.arctan2(y2 - y1, x2 - x1)
        if math.fabs(ang) <= 30 / 180 * math.pi:
            angle += ang
            cnt += 1

    if cnt == 0:
        return 0.0
    return (angle / cnt) * 180 / math.pi

def deskew(src_img):
    """L√†m th·∫≥ng ·∫£nh b·ªã nghi√™ng"""
    return rotate_image(src_img, compute_skew(src_img))

def preprocess_v3(oimg):
    """H√†m ti·ªÅn x·ª≠ l√Ω ·∫£nh  ƒë·ªÉ c·∫£i thi·ªán OCR"""
    try:
        dimg = deskew(oimg)
        img = cv2.cvtColor(dimg, cv2.COLOR_BGR2GRAY)
        _, img = cv2.threshold(img, 140, 255, cv2.THRESH_BINARY)
        img = cv2.GaussianBlur(img, (5, 5), 0)

        rectKern = cv2.getStructuringElement(cv2.MORPH_RECT, (6, 5))
        blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, rectKern)

        squareKern = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
        light = cv2.morphologyEx(img, cv2.MORPH_CLOSE, squareKern)
        light = cv2.threshold(light, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]

        rows, cols = np.where(light == 255)
        if rows.size == 0 or cols.size == 0:
            return oimg

        top, bottom = rows.min(), rows.max()
        left, right = cols.min(), cols.max()

        cropped = dimg[top:bottom, left:right]
        if cropped.size == 0:
            return oimg

        img = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)
        median_val = np.median(img.reshape(-1,), axis=0)
        mean_val = np.mean(img.reshape(-1,), axis=0)
        _, img = cv2.threshold(img, (median_val + mean_val) / 2, 255, cv2.THRESH_BINARY)
        img = cv2.GaussianBlur(img, (7, 7), 0)

        recolor = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)

        sharpened_image2 = cv2.addWeighted(cropped, 0.5, recolor, 0.5, 0)
        for i in range(3, 10, 2):
            blurred = cv2.GaussianBlur(sharpened_image2, (i, i), 1)
            sharpened_image2 = cv2.addWeighted(sharpened_image2, 1.5, blurred, -0.5, 0)
        
        return sharpened_image2
    except Exception as e:
        print(f"Error in preprocess_v3: {e}")
        return oimg


dict_char_to_int = {'O': '0',
                    'L': '4',
                    'B': '8',
                    'I': '1',
                    'T': '1',
                    'J': '3',
                    'A': '4',
                    'G': '6',
                    'Z': '2',
                    'S': '5'}

dict_int_to_char = {'0': 'C',
                    '1': 'T',
                    '4': 'A',
                    '6': 'G',
                    '2': 'Z',
                    '5': 'S',
                    '8': 'B'}


def remove_symbols(result):
    """X√≥a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát"""
    return re.sub(r'[^A-Z\d]', '', result)

def post_process(list_results, letter_idx=2):
    """H·∫≠u x·ª≠ l√Ω text nh·∫≠n di·ªán """
    predict_results = []
    for result in list_results:
        if len(result) < 2:
            predict_results.append(result)
            continue

        # D√≤ng tr√™n
        if len(result[0]) > letter_idx:
            letter = result[0][letter_idx]
            if letter in dict_int_to_char.keys():
                result[0] = result[0][:letter_idx] + dict_int_to_char[letter] + result[0][letter_idx+1:]

        for i in range(3):
            if i != letter_idx:
                if i < len(result[0]):
                    number = result[0][i]
                    if number in dict_char_to_int.keys():
                        result[0] = result[0][:i] + dict_char_to_int[number] + result[0][i+1:]

        if len(result[0]) >= 4:
            char_var = result[0][3]
            if char_var not in ["A", "B"]:
                if char_var in dict_char_to_int.keys():
                    result[0] = result[0][:3] + dict_char_to_int[char_var]

        # D√≤ng d∆∞·ªõi
        for i in range(len(result[1])):
            number = result[1][i]
            if number in dict_char_to_int.keys():
                result[1] = result[1].replace(number, dict_char_to_int[number])
        
        predict_results.append(result)
    return predict_results

# ---  H√†m x·ª≠ l√Ω ch√≠nh cho Gradio ---

def recognize_license_plate(input_image):
    """
    H√†m ch√≠nh th·ª±c hi·ªán to√†n b·ªô pipeline:
    1. Ph√°t hi·ªán T·∫§T C·∫¢ bi·ªÉn s·ªë b·∫±ng YOLO
    2. L·∫∑p qua t·ª´ng bi·ªÉn s·ªë:
        a. C·∫Øt v√πng bi·ªÉn s·ªë (crop)
        b. X·ª≠ l√Ω ·∫£nh (deskew, preprocess_v3)
        c. Nh·∫≠n di·ªán ch·ªØ b·∫±ng EasyOCR
        d. H·∫≠u x·ª≠ l√Ω text
    3. V·∫Ω bounding box v√† text l√™n ·∫£nh k·∫øt qu·∫£
    4. Tr·∫£ v·ªÅ: ·∫¢nh k·∫øt qu·∫£, danh s√°ch ·∫£nh ƒë√£ x·ª≠ l√Ω, danh s√°ch ·∫£nh ƒë√£ c·∫Øt, chu·ªói vƒÉn b·∫£n t·ªïng h·ª£p
    """
    if yolo_model is None or reader is None:
        # Tr·∫£ v·ªÅ 4 gi√° tr·ªã r·ªóng/m·∫∑c ƒë·ªãnh ƒë·ªÉ kh·ªõp v·ªõi outputs c·ªßa Gradio
        return input_image, None, None, "L·ªói: Model ch∆∞a ƒë∆∞·ª£c t·∫£i."

    # T·∫°o b·∫£n sao ƒë·ªÉ v·∫Ω l√™n
    image_with_boxes = input_image.copy()
    
    # 1. Ph√°t hi·ªán bi·ªÉn s·ªë b·∫±ng YOLO
    results = yolo_model.predict(
        input_image,
        imgsz=480,
        conf=0.5, 
        verbose=False
    )

    if not results or not results[0].boxes:
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ ngay c·∫£ khi kh√¥ng t√¨m th·∫•y g√¨
        gc.collect() 
        return input_image, None, None, "Kh√¥ng ph√°t hi·ªán th·∫•y bi·ªÉn s·ªë."

    # Kh·ªüi t·∫°o c√°c danh s√°ch ƒë·ªÉ l∆∞u k·∫øt qu·∫£
    all_cropped_plates = []
    all_processed_plates = []
    all_recognized_texts = []

    # 2. L·∫∑p qua T·∫§T C·∫¢ c√°c bi·ªÉn s·ªë ƒë∆∞·ª£c ph√°t hi·ªán
    for box in results[0].boxes:
        try:
            # 2a. C·∫Øt v√πng bi·ªÉn s·ªë
            coords = box.xyxy[0].cpu().numpy().astype(int)
            [x1, y1, x2, y2] = coords
            x1, y1, x2, y2 = max(0, x1), max(0, y1), max(0, x2), max(0, y2)
            
            cropped_plate = input_image[y1:y2, x1:x2]
            if cropped_plate.size == 0:
                continue # B·ªè qua n·∫øu ·∫£nh c·∫Øt b·ªã r·ªóng
            
            all_cropped_plates.append(cropped_plate)

            # 2b. X·ª≠ l√Ω ·∫£nh
            processed_plate = preprocess_v3(cropped_plate)
            all_processed_plates.append(processed_plate)

            # 2c. Nh·∫≠n di·ªán ch·ªØ
            allow_list = string.ascii_uppercase + string.digits + '-.'
            ocr_result_raw = reader.readtext(processed_plate, detail=0, allowlist=allow_list)

            recognized_text = "N/A" # Gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu OCR th·∫•t b·∫°i
            if ocr_result_raw:
                # 2d. H·∫≠u x·ª≠ l√Ω text
                cleaned_results = [remove_symbols(r.upper()) for r in ocr_result_raw]
                cleaned_results = [r for r in cleaned_results if r]

                if cleaned_results:
                    if len(cleaned_results) >= 2:
                        try:
                            lines_to_process = [cleaned_results[0], cleaned_results[1]]
                            final_result_list = post_process([lines_to_process])
                            recognized_text = "".join(final_result_list[0])
                        except Exception as e:
                            print(f"Post-processing failed: {e}. Falling back.")
                            recognized_text = cleaned_results[0] + cleaned_results[1]
                    else:
                        recognized_text = "".join(cleaned_results)

            all_recognized_texts.append(recognized_text)
            
            # 3. V·∫Ω bounding box v√† text l√™n ·∫£nh k·∫øt qu·∫£
            cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(image_with_boxes, recognized_text, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

        except Exception as e:
            print(f"Error processing box: {e}")
            continue # B·ªè qua box n√†y n·∫øu c√≥ l·ªói

    # 4. Chu·∫©n b·ªã chu·ªói vƒÉn b·∫£n t·ªïng h·ª£p ƒë·ªÉ tr·∫£ v·ªÅ
    if not all_recognized_texts:
        # Tr∆∞·ªùng h·ª£p YOLO ph√°t hi·ªán nh∆∞ng t·∫•t c·∫£ c√°c b∆∞·ªõc sau ƒë·ªÅu l·ªói
        gc.collect() 
        return input_image, None, None, "Ph√°t hi·ªán ƒë∆∞·ª£c bi·ªÉn s·ªë nh∆∞ng kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ch·ªØ."

    final_text_output = ""
    for i, txt in enumerate(all_recognized_texts):
        final_text_output += f"Bi·ªÉn s·ªë {i+1}: {txt}\n"

    # Ch·∫°y b·ªô d·ªçn r√°c th·ªß c√¥ng tr∆∞·ªõc khi tr·∫£ v·ªÅ k·∫øt qu·∫£
    gc.collect()

    # Tr·∫£ v·ªÅ 4 gi√° tr·ªã
    return image_with_boxes, all_processed_plates, all_cropped_plates, final_text_output

# ---  X√¢y d·ª±ng giao di·ªán Gradio ---
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ·ª®ng d·ª•ng nh·∫≠n di·ªán bi·ªÉn s·ªë xe üöó
        Upload ·∫£nh xe c·ªßa b·∫°n ƒë·ªÉ h·ªá th·ªëng ph√°t hi·ªán v√† nh·∫≠n di·ªán bi·ªÉn s·ªë.
        """
    )

    with gr.Row():
        image_input = gr.Image(type="numpy", label="Upload ·∫£nh t·∫°i ƒë√¢y")

    # Th√™m ·∫£nh m·∫´u t·ª´ th∆∞ m·ª•c imgs
    gr.Examples(
        examples=[
            "imgs/sample1.jpg",
            "imgs/sample2.jpg"
        ],
        inputs=image_input
    )

    recognize_button = gr.Button("Nh·∫≠n di·ªán üîé", variant="primary")

    with gr.Accordion("K·∫øt qu·∫£ chi ti·∫øt", open=True):
        # Output 1: ·∫¢nh k·∫øt qu·∫£ (v·∫Ω t·∫•t c·∫£ box)
        result_image_output = gr.Image(label="·∫¢nh k·∫øt qu·∫£ (T·∫•t c·∫£ bi·ªÉn s·ªë)")

        # Output 4: K·∫øt qu·∫£ nh·∫≠n di·ªán (d·∫°ng vƒÉn b·∫£n)
        text_output = gr.Textbox(label="K·∫øt qu·∫£ nh·∫≠n di·ªán (EasyOCR)", lines=5)

        with gr.Row():
            # Output 3: Gallery c√°c ·∫£nh ƒë√£ c·∫Øt (t·ª´ YOLO)
            cropped_gallery = gr.Gallery(label="C√°c v√πng bi·ªÉn s·ªë (YOLO crop)", columns=4, height=200)

            # Output 2: Gallery c√°c ·∫£nh ƒë√£ x·ª≠ l√Ω (cho OCR)
            processed_gallery = gr.Gallery(label="Bi·ªÉn s·ªë ƒë√£ x·ª≠ l√Ω (cho OCR)", columns=4, height=200)

    # Li√™n k·∫øt n√∫t b·∫•m v·ªõi h√†m x·ª≠ l√Ω
    recognize_button.click(
        fn=recognize_license_plate,
        inputs=image_input,
        outputs=[result_image_output, processed_gallery, cropped_gallery, text_output]
    )

    gr.Markdown("--- \n *Powered by Gradio, YOLOv, and EasyOCR*")

# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    if yolo_model is None or reader is None:
        print("Models failed to load. Gradio app will not start.")
    else:
        print("Starting Gradio app...")
        demo.launch(debug=True)